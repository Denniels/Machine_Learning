{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores de Máximo Margen\n",
    "\n",
    "### Competencias\n",
    "- Comprender el principio de la maximización del margen como un clasificador. \n",
    "### Introducción\n",
    "Las Máquinas de Soporte Vectorial (de aqui en adelante SVM, Support Vector Machines) son\n",
    "modelos con una amplia versatilidad para manejar tareas de clasificación y regresión en\n",
    "casos lineales y no lineales. Una de las principales bondades de SVM es su rápida\n",
    "implementación y buenos resultados predictivos, situándose entre las mejores técnicas\n",
    "junto a las redes neuronales, ensambles, sistemas de votación y gradient boosting\n",
    "(elementos que veremos posteriormente). Para entender el comportamiento de SVM,\n",
    "Murphy (2012) recalca que es una mezcla entre las técnicas kernel trick, sparsity y large\n",
    "margin principle. Cada una de éstas mantiene tanto sesgo como varianza controlado en\n",
    "niveles aceptables.\n",
    "\n",
    "### El caso separable: Clasificadores de Máximo Margen\n",
    "Para motivar el uso de SVM, necesitamos estudiar el principio del máximo margen. Éste\n",
    "surge de la incapacidad de los modelos logísticos de encontrar una frontera de decisión\n",
    "donde los atributos son linealmente separables. La figura izquierda generada con\n",
    "afx.setup_svm_problem() demuestra este problema. La figura presenta una serie de\n",
    "clasificadores lineales, que representan funciones de decisión candidatas para generar una\n",
    "separación eficiente. Lamentablemente algunos de los candidatos clasificadores tienden a\n",
    "acercarse a los atributos en algún rango, lo que imposibilita generar un clasificador óptimo.\n",
    "\n",
    "SVM ocupa el principio de máximo margen para encontrar un clasificador lineal óptimo. Este\n",
    "se visualiza en la figura derecha de afx.setup_svm_problem(). La obtención del\n",
    "clasificador lineal para este caso implica **`maximizar`** el margen en la distancia entre las dos\n",
    "nubes de datos. Para ello, el algoritmo se vale de los vectores de soporte (encapsulados en\n",
    "un círculo) que establecerán el ancho del margen, en función al candidato lineal β𝑇 + β0 = 0.\n",
    "\n",
    "**`El principio del máximo margen es bastante intuitivo:`** Si un clasificador tiene una frontera\n",
    "de clasificación demasiado cerca de las instancias de las clases, muy probablemente\n",
    "cuando se agreguen nuevas instancias de datos, algunas de estas caigan del otro lado de la\n",
    "frontera que les corresponde haciendo el clasificador inutil. Por otro lado, si maximizamos la\n",
    "distancia entre las clases y la frontera (el margen entre las clases), aunque se agreguen más\n",
    "datos estos rara vez se deberían desviar demasiado de los datos ya vistos. Por lo tanto,\n",
    "probablemente estarán dentro de las fronteras correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import lec5_graphs as afx\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize']=(10, 10)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afx.setup_svm_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digresión: Obtención del plano clasificador en el caso lineal separable\n",
    "\n",
    "Si tenemos **`N`** observaciones en pares de forma $(x_1, y_2),...(x_n, y_n)$ donde $x_1 \\in X^R$ e $y \\in Y \\to \\{-1, 1\\}$\n",
    "se busca un hiperplano con la siguiente expresion:\n",
    "$\\{x:f(x) = X^T\\beta + \\beta_0 = 0\\}$\n",
    "\n",
    "A partir de este hiperplano buscamos una regla de clasificación $G(x) = sign[X\\beta + \\beta_0]$ donde **`G(x)`**  da la distancia de 𝑥 en el hiperplano.\n",
    "\n",
    "Posterior a la obtención de éste candidato, buscamos maximizar la distancia del márgen\n",
    "con la siguiente estrategia de optimización:\n",
    "\n",
    "$$argmax M$$\n",
    "$$\\beta,\\beta_0||\\beta|| = 1$$\n",
    "$$Subjeto a: y_i(X^T\\beta + \\beta_0)\\geq M \\hspace{20mm} \\forall_i \\in N$$\n",
    "\n",
    "Donde **`M`** es el margen, en la figura de derecha afx.setup_svm_problem() el margen **`M`** es\n",
    "la distancia entre cualquiera de las líneas punteadas y la línea central continua que\n",
    "representa la frontera de clasificación.\n",
    "\n",
    "Éste caso hipotético donde las clases son linealmente separables se conoce como\n",
    "**`clasificación estricta de margen`**, dado que la optimización de la función dependerá\n",
    "exclusivamente que dentro del margen no existan vectores. Otro aspecto a considerar es el\n",
    "hecho que los clasificadores estrictos **`son sensibles a outliers`**, por lo que la obtención de\n",
    "márgenes en una muestra de entrenamiento con outliers puede conllevar a fallas de\n",
    "generalización del modelo. El principal problema de esta aproximación es que si las clases\n",
    "no son linealmente separables el problema no tiene solución, es decir, nuestro modelo no es\n",
    "capaz de encontrar ningún clasificador.\n",
    "\n",
    "### Digresión: Estandarización de atributos\n",
    "Para evitar la influencia de outliers en la muestra de entrenamiento, podemos preprocesar\n",
    "los datos mediante StandardScaler. También existe la posibilidad de preprocesar los\n",
    "atributos con alguna técnica de reducción de dimensiones como PCA, con la salvedad que\n",
    "SVM hace uso de kernels para reexpresar la dimensionalidad de los datos. Implementar PCA\n",
    "en algunos casos puede forzar linealidad, cosa que no deseamos. Posteriormente\n",
    "hablaremos de los kernels.\n",
    "\n",
    "El objetivo de SVM es generar un caso de **`clasificación flexible de margen`**, donde\n",
    "permitimos encontrar una frontera de decisión optimizada que permita que algunos puntos\n",
    "puedan posicionarse dentro del margen. Este caso da origen a la discusión de las máquinas\n",
    "de soporte vectorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fe23d4cdea1a1fdeea398f38169f58ea6e36b10f84ee4017a8f0fee693ee786"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
