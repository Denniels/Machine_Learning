{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores de M谩ximo Margen\n",
    "\n",
    "### Competencias\n",
    "- Comprender el principio de la maximizaci贸n del margen como un clasificador. \n",
    "### Introducci贸n\n",
    "Las M谩quinas de Soporte Vectorial (de aqui en adelante SVM, Support Vector Machines) son\n",
    "modelos con una amplia versatilidad para manejar tareas de clasificaci贸n y regresi贸n en\n",
    "casos lineales y no lineales. Una de las principales bondades de SVM es su r谩pida\n",
    "implementaci贸n y buenos resultados predictivos, situ谩ndose entre las mejores t茅cnicas\n",
    "junto a las redes neuronales, ensambles, sistemas de votaci贸n y gradient boosting\n",
    "(elementos que veremos posteriormente). Para entender el comportamiento de SVM,\n",
    "Murphy (2012) recalca que es una mezcla entre las t茅cnicas kernel trick, sparsity y large\n",
    "margin principle. Cada una de 茅stas mantiene tanto sesgo como varianza controlado en\n",
    "niveles aceptables.\n",
    "\n",
    "### El caso separable: Clasificadores de M谩ximo Margen\n",
    "Para motivar el uso de SVM, necesitamos estudiar el principio del m谩ximo margen. ste\n",
    "surge de la incapacidad de los modelos log铆sticos de encontrar una frontera de decisi贸n\n",
    "donde los atributos son linealmente separables. La figura izquierda generada con\n",
    "afx.setup_svm_problem() demuestra este problema. La figura presenta una serie de\n",
    "clasificadores lineales, que representan funciones de decisi贸n candidatas para generar una\n",
    "separaci贸n eficiente. Lamentablemente algunos de los candidatos clasificadores tienden a\n",
    "acercarse a los atributos en alg煤n rango, lo que imposibilita generar un clasificador 贸ptimo.\n",
    "\n",
    "SVM ocupa el principio de m谩ximo margen para encontrar un clasificador lineal 贸ptimo. Este\n",
    "se visualiza en la figura derecha de afx.setup_svm_problem(). La obtenci贸n del\n",
    "clasificador lineal para este caso implica **`maximizar`** el margen en la distancia entre las dos\n",
    "nubes de datos. Para ello, el algoritmo se vale de los vectores de soporte (encapsulados en\n",
    "un c铆rculo) que establecer谩n el ancho del margen, en funci贸n al candidato lineal 尾 + 尾0 = 0.\n",
    "\n",
    "**`El principio del m谩ximo margen es bastante intuitivo:`** Si un clasificador tiene una frontera\n",
    "de clasificaci贸n demasiado cerca de las instancias de las clases, muy probablemente\n",
    "cuando se agreguen nuevas instancias de datos, algunas de estas caigan del otro lado de la\n",
    "frontera que les corresponde haciendo el clasificador inutil. Por otro lado, si maximizamos la\n",
    "distancia entre las clases y la frontera (el margen entre las clases), aunque se agreguen m谩s\n",
    "datos estos rara vez se deber铆an desviar demasiado de los datos ya vistos. Por lo tanto,\n",
    "probablemente estar谩n dentro de las fronteras correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import lec5_graphs as afx\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize']=(10, 10)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afx.setup_svm_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digresi贸n: Obtenci贸n del plano clasificador en el caso lineal separable\n",
    "\n",
    "Si tenemos **`N`** observaciones en pares de forma $(x_1, y_2),...(x_n, y_n)$ donde $x_1 \\in X^R$ e $y \\in Y \\to \\{-1, 1\\}$\n",
    "se busca un hiperplano con la siguiente expresion:\n",
    "$\\{x:f(x) = X^T\\beta + \\beta_0 = 0\\}$\n",
    "\n",
    "A partir de este hiperplano buscamos una regla de clasificaci贸n $G(x) = sign[X\\beta + \\beta_0]$ donde **`G(x)`**  da la distancia de  en el hiperplano.\n",
    "\n",
    "Posterior a la obtenci贸n de 茅ste candidato, buscamos maximizar la distancia del m谩rgen\n",
    "con la siguiente estrategia de optimizaci贸n:\n",
    "\n",
    "$$argmax M$$\n",
    "$$\\beta,\\beta_0||\\beta|| = 1$$\n",
    "$$Subjeto a: y_i(X^T\\beta + \\beta_0)\\geq M \\hspace{20mm} \\forall_i \\in N$$\n",
    "\n",
    "Donde **`M`** es el margen, en la figura de derecha afx.setup_svm_problem() el margen **`M`** es\n",
    "la distancia entre cualquiera de las l铆neas punteadas y la l铆nea central continua que\n",
    "representa la frontera de clasificaci贸n.\n",
    "\n",
    "ste caso hipot茅tico donde las clases son linealmente separables se conoce como\n",
    "**`clasificaci贸n estricta de margen`**, dado que la optimizaci贸n de la funci贸n depender谩\n",
    "exclusivamente que dentro del margen no existan vectores. Otro aspecto a considerar es el\n",
    "hecho que los clasificadores estrictos **`son sensibles a outliers`**, por lo que la obtenci贸n de\n",
    "m谩rgenes en una muestra de entrenamiento con outliers puede conllevar a fallas de\n",
    "generalizaci贸n del modelo. El principal problema de esta aproximaci贸n es que si las clases\n",
    "no son linealmente separables el problema no tiene soluci贸n, es decir, nuestro modelo no es\n",
    "capaz de encontrar ning煤n clasificador.\n",
    "\n",
    "### Digresi贸n: Estandarizaci贸n de atributos\n",
    "Para evitar la influencia de outliers en la muestra de entrenamiento, podemos preprocesar\n",
    "los datos mediante StandardScaler. Tambi茅n existe la posibilidad de preprocesar los\n",
    "atributos con alguna t茅cnica de reducci贸n de dimensiones como PCA, con la salvedad que\n",
    "SVM hace uso de kernels para reexpresar la dimensionalidad de los datos. Implementar PCA\n",
    "en algunos casos puede forzar linealidad, cosa que no deseamos. Posteriormente\n",
    "hablaremos de los kernels.\n",
    "\n",
    "El objetivo de SVM es generar un caso de **`clasificaci贸n flexible de margen`**, donde\n",
    "permitimos encontrar una frontera de decisi贸n optimizada que permita que algunos puntos\n",
    "puedan posicionarse dentro del margen. Este caso da origen a la discusi贸n de las m谩quinas\n",
    "de soporte vectorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fe23d4cdea1a1fdeea398f38169f58ea6e36b10f84ee4017a8f0fee693ee786"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
